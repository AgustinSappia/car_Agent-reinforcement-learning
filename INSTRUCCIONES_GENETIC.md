# Sistema de Entrenamiento GenÃ©tico/Evolutivo

## ğŸš€ CaracterÃ­sticas Principales

### 1. **MÃºltiples Agentes SimultÃ¡neos**
- Entrena varios agentes al mismo tiempo en la misma pista
- Cada agente tiene su propio color para fÃ¡cil identificaciÃ³n
- Los agentes compiten entre sÃ­ en cada generaciÃ³n

### 2. **SelecciÃ³n Evolutiva**
- Al final de cada generaciÃ³n, se seleccionan los mejores agentes
- Los mejores se clonan y mutan para crear la siguiente generaciÃ³n
- Sistema de elitismo: los mejores agentes pasan directamente a la siguiente generaciÃ³n

### 3. **MenÃº Interactivo en Tiempo Real**
Ubicado en la esquina superior derecha, permite ajustar:
- **NÃºmero de Agentes** (1-8): Cantidad de agentes por generaciÃ³n
- **Max Reward** (50-500): Recompensa mÃ¡xima objetivo
- **Epsilon** (0.0-1.0): Nivel de exploraciÃ³n aleatoria
- **Velocidad** (0.5-5.0): Multiplicador de velocidad de simulaciÃ³n

### 4. **VisualizaciÃ³n Mejorada**
- Cada agente muestra su ID y fitness actual
- Colores distintos para cada agente
- Panel de control con estadÃ­sticas en tiempo real

---

## ğŸ“‹ CÃ³mo Usar

### Ejecutar el Entrenamiento

```bash
python train_genetic.py
```

### Controles

| Tecla | AcciÃ³n |
|-------|--------|
| **SPACE** | Pausar/Reanudar entrenamiento |
| **ESC** | Salir del entrenamiento |
| **Click Izquierdo** | Ajustar parÃ¡metros en el menÃº |

### Ajustar ParÃ¡metros

1. **NÃºmero de Agentes**: Click en +/- para cambiar cuÃ¡ntos agentes entrenan simultÃ¡neamente
2. **Epsilon**: Controla la exploraciÃ³n (0.0 = sin exploraciÃ³n, 1.0 = mÃ¡xima exploraciÃ³n)
3. **Velocidad**: Acelera o desacelera la simulaciÃ³n (Ãºtil para entrenamientos largos)

---

## ğŸ§¬ CÃ³mo Funciona el Algoritmo GenÃ©tico

### 1. **InicializaciÃ³n**
- Se crean N agentes con redes neuronales aleatorias
- Cada agente tiene su propio "cerebro" (red neuronal)

### 2. **EvaluaciÃ³n (Fitness)**
- Cada agente conduce hasta chocar o alcanzar el lÃ­mite de pasos
- El fitness se calcula como: `fitness = recompensa_total + pasos * 0.1`
- Mayor fitness = mejor desempeÃ±o

### 3. **SelecciÃ³n**
- Los agentes se ordenan por fitness
- Los mejores 25% pasan directamente (elitismo)
- El resto se elimina

### 4. **ReproducciÃ³n y MutaciÃ³n**
- Los mejores agentes se clonan
- Los clones se mutan (cambios aleatorios en los pesos de la red)
- Esto crea variedad genÃ©tica

### 5. **Nueva GeneraciÃ³n**
- Los agentes elite + los mutados forman la nueva generaciÃ³n
- El proceso se repite

---

## ğŸ“Š Sistema de Recompensas

### Recompensas Positivas
- **Velocidad**: `reward = velocidad / velocidad_mÃ¡xima`
- **Supervivencia**: `+0.1` por cada paso sin chocar

### Penalizaciones
- **ColisiÃ³n**: `-100.0` (termina el episodio)
- **Proximidad a bordes**: `-0.5` si sensor < 20% de distancia mÃ¡xima

### Fitness Total
```
fitness = recompensa_total + (pasos * 0.1)
```

---

## ğŸ’¾ Archivos Generados

### `best_genetic_agent.pth`
- Mejor agente de todas las generaciones
- Se guarda automÃ¡ticamente cuando se supera el rÃ©cord
- Se carga automÃ¡ticamente al iniciar

### `genetic_training_log.csv`
Registro de entrenamiento con columnas:
- `Generation`: NÃºmero de generaciÃ³n
- `Best_Fitness`: Mejor fitness de la generaciÃ³n
- `Avg_Fitness`: Fitness promedio
- `Num_Agents`: NÃºmero de agentes
- `Epsilon`: Valor de epsilon usado

---

## ğŸ¨ Colores de los Agentes

| Color | Agente |
|-------|--------|
| ğŸ”´ Rojo | Agente #0 (usualmente el mejor) |
| ğŸŸ¢ Verde | Agente #1 |
| ğŸ”µ Azul | Agente #2 |
| ğŸŸ¡ Amarillo | Agente #3 |
| ğŸŸ£ Magenta | Agente #4 |
| ğŸ”· Cyan | Agente #5 |
| ğŸŸ  Naranja | Agente #6 |
| ğŸŸ£ PÃºrpura | Agente #7 |

---

## âš™ï¸ ParÃ¡metros Recomendados

### Para Entrenamiento RÃ¡pido
- **Agentes**: 3-4
- **Epsilon**: 0.2-0.3
- **Velocidad**: 2.0-3.0

### Para Mejor Calidad
- **Agentes**: 6-8
- **Epsilon**: 0.1-0.15
- **Velocidad**: 1.0

### Para ExploraciÃ³n
- **Agentes**: 5
- **Epsilon**: 0.5-0.8
- **Velocidad**: 1.5

---

## ğŸ”§ Diferencias con train_dqn.py

| CaracterÃ­stica | train_dqn.py | train_genetic.py |
|----------------|--------------|------------------|
| **MÃ©todo** | Deep Q-Learning | Algoritmo GenÃ©tico |
| **Agentes** | 1 a la vez | MÃºltiples simultÃ¡neos |
| **Aprendizaje** | Gradiente descendente | EvoluciÃ³n/MutaciÃ³n |
| **Memoria** | Experience Replay | No usa memoria |
| **Velocidad** | MÃ¡s lento | MÃ¡s rÃ¡pido |
| **ExploraciÃ³n** | Epsilon-greedy | MutaciÃ³n genÃ©tica |
| **Mejor para** | Convergencia precisa | ExploraciÃ³n rÃ¡pida |

---

## ğŸ“ˆ Consejos para Mejor Entrenamiento

### 1. **Ajusta Epsilon Gradualmente**
- Empieza con epsilon alto (0.3-0.5) para exploraciÃ³n
- Reduce gradualmente a 0.1-0.15 cuando veas progreso

### 2. **NÃºmero de Agentes**
- MÃ¡s agentes = mÃ¡s diversidad pero mÃ¡s lento
- 5-6 agentes es un buen balance

### 3. **Velocidad de SimulaciÃ³n**
- Usa velocidad alta (3.0-5.0) cuando el entrenamiento es estable
- Reduce a 1.0 para observar comportamiento detallado

### 4. **Paciencia**
- Las primeras 10-20 generaciones pueden ser caÃ³ticas
- El progreso real suele verse despuÃ©s de 30-50 generaciones

### 5. **Guarda Progreso**
- El mejor agente se guarda automÃ¡ticamente
- Puedes detener y reanudar el entrenamiento sin perder progreso

---

## ğŸ› SoluciÃ³n de Problemas

### Los agentes chocan inmediatamente
- **SoluciÃ³n**: Aumenta epsilon a 0.3-0.5 para mÃ¡s exploraciÃ³n
- Verifica que la posiciÃ³n inicial estÃ© en la pista blanca

### El entrenamiento es muy lento
- **SoluciÃ³n**: Aumenta la velocidad de simulaciÃ³n
- Reduce el nÃºmero de agentes

### No hay progreso despuÃ©s de muchas generaciones
- **SoluciÃ³n**: Aumenta epsilon temporalmente
- Reinicia con el mejor agente guardado
- Ajusta las recompensas en el cÃ³digo si es necesario

### La ventana se congela
- **SoluciÃ³n**: Los eventos de pygame se procesan correctamente
- Si persiste, reduce la velocidad de simulaciÃ³n

---

## ğŸ¯ Objetivos de Fitness

| Fitness | Nivel |
|---------|-------|
| < 50 | Principiante (choca rÃ¡pido) |
| 50-200 | BÃ¡sico (sobrevive un poco) |
| 200-500 | Intermedio (completa parte del circuito) |
| 500-1000 | Avanzado (completa varias vueltas) |
| > 1000 | Experto (conduce establemente) |

---

## ğŸ“ Notas TÃ©cnicas

### Arquitectura de Red Neuronal
- **Entrada**: 7 valores (5 sensores + velocidad + Ã¡ngulo)
- **Capas ocultas**: Definidas en `model.py`
- **Salida**: 4 acciones (nada, acelerar, izq+acel, der+acel)

### MutaciÃ³n
- **Tasa de mutaciÃ³n**: 20% de los pesos
- **Fuerza de mutaciÃ³n**: Â±30% del valor original
- Usa distribuciÃ³n normal para cambios suaves

### Elitismo
- 25% de los mejores agentes pasan sin cambios
- Garantiza que no se pierda el mejor desempeÃ±o

---

## ğŸš€ PrÃ³ximos Pasos

1. Ejecuta `python train_genetic.py`
2. Observa las primeras generaciones
3. Ajusta parÃ¡metros segÃºn el comportamiento
4. Deja entrenar por 50-100 generaciones
5. El mejor agente se guarda automÃ¡ticamente

Â¡Buena suerte con el entrenamiento! ğŸï¸ğŸ’¨
